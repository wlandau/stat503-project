---
title: "What distinguishes high-performing students?"
author: Will Landau
date: April 25, 2015
output: 
  pdf_document:
    fig_caption: true
    number_sections: true
bibliography: report.bib
---

```{r, echo=FALSE}
set.seed(0)
library(knitr)
knitr::knit_hooks$set(inline = as.character)
knitr::opts_chunk$set(cache=T, fig.height = 4, echo = F, results="hide", message = F)
knitr::opts_knit$set(eval.after = 'fig.cap')
```

```{r sources}
for(f in paste("../R/", c(
  "issue-groupings",
  "getData",
  "explore",
  "explore_matching",
  "explore_imputed"
), ".R", sep=""))
  source(f)
```

# Introduction 

Which factors best separate successful students from those who struggle? How well can we predict academic success using  conditions we can observe and control? For insight, I look at data from the Organization for Economic Co-operation and Development (OECD). In 2012, The OECD's Programme for International Student Assessment (PISA) surveyed roughly five hundred thousand, fifteen-year-old students from sixty-five economies across the globe [@oecd]. Questions measured students' reading, math, and science skills with examinations that, according to the OECD website, "are not directly linked to the school curriculum. The tests are designed to assess to what extent students at the end of compulsory education, can apply their knowledge to real-life situations and be equipped for full participation in society" [@oecd]. Students also answered extensive background questionnaire about their study habits, attitudes towards school, circumstances at home, etc., al of which are factors that may influence student success. In the analysis below, I derive a "student success" variable from the reading and math scores and attempt to predict success using information from the background questionnaire.



# A first cleanup: getting ready to explore

The student-specific 2012 PISA dataset is large and messy, and it needs to be cleaned and subsetted both before and after exploratory analysis. For example, to save computing time, and because overall pedagogy and the survey's implementation are different among different countries, only students from the United States will be examined here.

## Variables for predicting success

There are around 500 variables from the student background questionnaire, and large fraction of the answers are missing. In fact, after removing the few continuous survey variables and the questions with no recorded responses at all, only 256 variables are left. Of those 256, I remove the ones that probably cannot help education policy, such as self-efficacy measures, self-reported prior familiarity and experience with math and reading concepts, and nondescript "ISCED" variables. 210 factor variables remain for prediction, most of which have between 2 and 4 levels each.


## Measuring student success

For each student, the PISA dataset has 5 overall reading scores and 5 overall math scores. Each score is roughly on a continuous scale from around 200 to around 800, and as seen in Figure \ref{fig:pv}, the scores are highly correlated. Standardized test scores are only rough measures of academic performance, but when properly censored, they do expose the most egregious achievement gaps. To censor the data, I

1. Compute a total score for each student by summing the 10 standardized PISA scores together.
2. Collect the students with total scores above the 75th percentile, and call them "high-performing".
3. Collect the students with total scores below the 25th percentile, and call them "low-performing".
4. Remove the rest of the students from the data.

In the context of prediction, I now have a response variable with two possible values: high and low. I will temporarily suspend my skepticism and treat this factor as the gold standard of student success. 

```{r pv, fig.cap="\\label{fig:pv}histogram of pairwise correlations among the original 5 reading and 5 math scores from the PISA tests. Correlations are high, so I do not lose much information in summing them up to produce a single total score for each student."}
  pvcor()
```


# The best factors for predicting success

In this section, I look for the factors, conditions, general issues, etc. that best separate successful students from those who struggle.

## Ranking individual predictor variables

To get a rough picture of the important issues, I first rank all 210 variables individually. For the rankings, I use a matching heuristic that loosely measures how well a factor can split students by success level. For each factor $x$, I calculate this heuristic as follows. 

1. Remove the missing values from $x$, along with the corresponding values from the binary vector $y$ of student performances (high and low coded as 1 and 0, respectively).
2. For every subset $s$ of the levels of $x = (x_1, \ldots, x_n)$,
    a. Create the binary vector $z = (z_1, \ldots, z_n)$, where $z_i = I(x_i \in s)$.
    b. Let the matching score of $s$ be
\begin{align*}
\frac{1}{n} \max \left \{\sum_{i=1}^n I(y_i = z_i), \ \sum_{i=1}^n I(y_i \ne z_i) \right \}
\end{align*}

3. Take the matching score of $x$ to be the maximum of all the matching scores calculated in step 2. 


One can interpret the matching heuristic as the most optimistic rate of correct classification for a prediction on a single variable. A matching of 1 means that $x$ can predict $y$ perfectly, and a matching of 0.5 means that $x$ is no better than chance.

Figure \ref{fig:allmatchings} shows the matching heuristics of the 210 predictor variables. Most individual variables predict better than chance. The two variables with matchings better than 0.7 are "How many books at home" (censored) and "Vignette Classroom Management - Students Frequently Interrupt/Teacher Arrives Late".

```{r allmatchings, fig.cap="\\label{fig:allmatchings} matching heuristics of all 210 predictor variables. Most individual variables predict better than chance. The two variables with matchings better than 0.7 are \"How many books at home\" (censored) and \"Vignette Classroom Management - Students Frequently Interrupt/Teacher Arrives Late\"."}
matchingHist()
```

Figure \ref{fig:issue} shows the matching scores of the 210 variables, but this time, the variables are grouped by the general issues they cover, such as possessions, attitude, and parental backgrounds. Here, we can get a general picture of which issues matter in the sample of USA students. Academic habits outside school, school-related possessions, and attendance/truancy seem to be most related to performance, although there are few variables on these topics. There is a lot of variety among the attitude and teaching variables, and most of the survey questions seem to cover these issues. Many attitude and teaching questions are important, and will be used in the classifiers developed later. Interestingly, gender, course-content, and sociality (variables that attempt to measure the quality of students' social lives and social norms about school) were not very important. The parent-related variables seem largely split two ways, but this split does not separate mother-related variables from father-related ones.

```{r issue, fig.height=6, fig.cap="\\label{fig:issue} matching scores of the 210 variables, but this time, with the variables grouped by the general issues they cover, such as possessions, attitude, and parental backgrounds. Here, we can get a general picture of which issues matter in the sample of USA students. Academic habits outside school, school-related possessions, and attendance/truancy seem to be most related to performance, although there are few variables on these topics. There is a lot of variety among the attitude and teaching variables, and most of the survey questions seem to cover these issues. Many attitude and teaching questions are important, and will be used in the classifiers developed later. Interestingly, gender, course-content, and sociality (variables that attempt to measure the quality of students' social lives and social norms about school) were not very important. The parent-related variables seem largely split two ways, but this split does not separate mother-related variables from father-related ones."}
explore_by_issue()
```

## Variable selection and imputation

Figure \ref{fig:keepvars} shows the variables with the top 20 matching scores calculated in the previous section. I will use these to build a classifier.

```{r keepvars, fig.cap="\\label{fig:keepvars} variables with the top 20 matching scores. I will use these to build classifiers."}
student.factor.matchings.plot(n = 20, y.arg = "Description")
```

Unfortunately, even with a reduction in the number of variables, there are still a lot of missing values. Figure \ref{fig:studentmiss} show the number of missing values for each student, and Figure \ref{fig:missing} shows the percentage of missing values within each variable. The figures lead to the following imputation strategy:


```{r studentmiss, fig.cap="\\label{fig:studentmiss} number of missing values for each student (after selecting 20 variables to build a classifier)."}
missingByStudent()
```

```{r missing, fig.cap="\\label{fig:missing} Histogram of the percentage of missing values within each of the 20 selected predictor variables. Most variables are missing about a third of their values."}
missing.hist()
```

1. Remove the students who missed more than 13 questions (only 3.4\% of USA students). The resulting dataset 5052 students and no missing values. In Figure \ref{fig:moremissing}, I determine that no more students need to be removed for the imputation to be reasonable. 
2. Put all factors on a numeric scale such that the natural ordering of factor levels is preserved. (All factors are ordinal.) Center and scale the predictor variables, and denote student performance by 1 and -1 for high-performing and low-performing students, respectively.
3. Impute the remaining missing values with nearest neighbor imputation on the 20 predictor variables. I use the ``knnImputation`` function in the ``DMwR`` [@DMwR] package (setting the number of neighbors to 10).


```{r moremissing, fig.width = 8, fig.height = 8, fig.cap = "\\label{fig:moremissing} Before imputation, I removed all the students with more than 15 missing values. The purpose of this plot is to determine if I should have reduced this per-student missing value threshold to 11. In each panel, I plot a predictor variable for students with more than 11 missing values and for students with less than 12 missing values. For each variable, the ranges of the variables are nearly the same, and most of the corresponding quartiles are nearly the same. Hence, the imputation succeeded, and I do not need to remove any more students."}
more.missing.parcoord()
```




## Exploring the imputed data


Figure \ref{fig:var_hist} shows histograms of all the predictor variables. The nearest-neighbor imputation smoothed out many of the values so that there is some appearance of a continuous scale. Still, the original variables were discrete, so we see 2 to 4 peaks for each histogram. Model-based clustering with a mixture of normal distributions would not be prudent for this data.

```{r var_hist, fig.width = 8, fig.height = 8, fig.cap = "\\label{fig:var_hist} histograms of all the predictor variables. The nearest-neighbor imputation smoothed out many of the values so that there is some appearance of a continuous scale. Still, the original variables were discrete, so we see 2 to 4 peaks for each histogram. Model-based clustering with a mixture of normal distributions would not be prudent for this data."}
var.hist()
```


Figure \ref{fig:response_parcoord} plots each predictor variable versus student performance (high or low). All of the variables are associated with student performance, and for some variables, performance-specific interquartile regions do not overlap. Figure \ref{fig:varcor} shows the pairwise correlations among each of the predictor variables. Most correlations are acceptably close to zero. It seems reasonable to keep all 20 predictor variables.

```{r varcor, fig.cap = "\\label{fig:varcor} pairwise correlations among each of the predictor variables. Most correlations are acceptably close to zero."}
varcor()
```

```{r response_parcoord, fig.width = 8, fig.height = 8, fig.cap = "\\label{fig:response_parcoord} plots of each predictor variable versus student performance (high or low). All of the variables are associated with student performance, and for some variables, performance-specific interquartile regions do not overlap. "}
response.parcoord()
```

20 predictor variables can be cumbersome all together, so I used multidimensional scaling to see how the data separate. The ``mds`` function in the ``vegan`` package stalled when I tried to use the full dataset, so I took a random subset of 250 students. I plot the MDS components in Figure \ref{fig:mds}, and I color the points by student performance. The data almost separate by student performance. However, the separation is not complete, and in fact, little to no natural separation or clustering in the data is visible from this plot. 

```{r mds, fig.cap = "\\label{fig:mds} MDS components, where points (students) are colored by performance (high or low). The data almost separate by student performance. However, the separation is not complete, and in fact, little to no natural separation or clustering in the data is visible from this plot. "}
mds.plot()
```

# Plan for further work


- Run a basic clustering analysis on the 20 predictor variables in the imputed data. I will use kmeans and hierarchical clustering, and I will determine how much information about student performance these techniques recover.
- Build a classifier on the preprocessed and imputed data using:
    - Logistic regression
    - Neural networks
    - Random forests
    - Nearest neighbors classification




# Acknowledgements

I would like to thank Dr. Cook for steering me in the right direction. The PISA data is messy and cumbersome, and the guidance is very appreciated.

# References
